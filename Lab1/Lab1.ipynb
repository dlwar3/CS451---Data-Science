{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instruction\n",
    "Plase use <b>Python 3</b> in Jupyter Notebook.\n",
    "\n",
    "This lab focuses on classifcation and prediction. We will practice classification methods on a real world wetland mapping dataset. Each data sample contains several numeric features and a binary class label (0 for dry land, 1 for wetland). \n",
    "\n",
    "<b>Requirement</b>\n",
    "- <font color=red>Plese upload your Jupyter Notebook with required Data files and Python script files all in the SAME zipped FOLDER</font>\n",
    "\n",
    "- Please MAKE SURE your codes run smoothly without bugs in Jupyter Notebook with Python 3. \n",
    "    \n",
    "- <font color=red>Codes with bugs or errors that cannot run through in Jupyter notebook will be graded as ZERO for that part.</font> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\n",
    "from sklearn.linear_model import LogisticRegression # Import Logistic Regression\n",
    "from sklearn.svm import SVC # Import Support Vector Classifier\n",
    "from sklearn.ensemble import BaggingClassifier # Import Bagging Classifier \n",
    "from sklearn.ensemble import RandomForestClassifier # Import Random Forest Classifier\n",
    "from sklearn.model_selection import train_test_split # Import train_test_split function\n",
    "from sklearn import metrics #Import scikit-learn metrics module for accuracy calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading\n",
    "Please use the following codes to load the data. The training and test data are both saved in common separated values (CSV) format. The last column is class label. \n",
    "\n",
    "<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   near infra red   red  green  blue  class\n",
      "0              123  132    115   133      0\n",
      "1              152  150    119   187      1\n",
      "2              169  166    143   192      1\n",
      "3               55   49     43    97      0\n",
      "4              141  135    117   181      1\n",
      "   near infra red   red  green  blue  class\n",
      "0              137  140    129   150      0\n",
      "1              169  162    140   193      1\n",
      "2              124  110     89   162      1\n",
      "3              105  104     99   153      1\n",
      "4              105  102     88   173      1\n"
     ]
    }
   ],
   "source": [
    "#note: class 0 for dry land, class 1 for wetland\n",
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "# load dataset\n",
    "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
    "print(train_dat.head())\n",
    "\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "print(test_dat.head())\n",
    "\n",
    "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
    "X_train = train_dat.drop('class', axis=1)\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
    "X_test = test_dat.drop('class', axis=1)\n",
    "Y_test = test_dat['class']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Evaluate the performance of decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the codes below to train a decision tree, and make predictions on test samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Decision Tree classifer object\n",
    "dt = DecisionTreeClassifier()\n",
    "\n",
    "# Train Decision Tree Classifer\n",
    "dt = dt.fit(X_train,Y_train)\n",
    "\n",
    "#Predict the response for test dataset\n",
    "Y_pred = dt.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font> Please compute the \"Overall Accuracy\", as well as Precision, Recall, and F-score for the Wetland class (class 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.775\n",
      "0.7312775330396476\n",
      "0.7632183908045977\n",
      "0.7469066366704162\n"
     ]
    }
   ],
   "source": [
    "# Fill in codes to calculate the values below; You can add any codes, but don't change the variable names\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision_wet)\n",
    "print(recall_wet)\n",
    "print(F_wet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>ANSWER THE QUESTION BELOW.</font> How do you think about the performance shown by different metrics above. Is accuracy a good metric to reflect classification performance? Why? You can discuss your answer as a string. And print it out. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Following is how we should be thinking about the performance metrics of accuracy, precision, recall, and F1.\n",
      "\n",
      "For accuracy, this is a calculation of set of classification labels that exactly match those of y_true from \n",
      "y_pred. This is the same as the jaccard score in binary classification.\n",
      "\n",
      "For precision, this is the number of true positives out of all positives, both true and false. This is how often the\n",
      "model's predictions are correct from all of the positive predictions that it makes. This can show when we are making incorrect\n",
      "true predictions.\n",
      "\n",
      "For recall, this is the number of true positives out of all true positives and false negatives. This calculates how many times\n",
      "our model predicts a positive correctly out of all actual positives. This can show when we are missing true predictions. \n",
      "\n",
      "For f1 score, this is a function of both precision and recall. This helps when there is a need to balance precision and recall\n",
      "while their is an uneven class distribution. \n",
      "\n",
      "Accuracy is not a good metric to measure classification performance. If 77.5% of the data set is class 1 \n",
      "and the decision tree predicts class 1 100% of the time, this would also lead to 0.775 accuracy score but with a \n",
      "poor prediction model. Therefore, accuracy is not a good metric to reflect classification performance.  \n"
     ]
    }
   ],
   "source": [
    "answer_1 = \"\"\"Following is how we should be thinking about the performance metrics of accuracy, precision, recall, and F1.\n",
    "\n",
    "For accuracy, this is a calculation of set of classification labels that exactly match those of y_true from \n",
    "y_pred. This is the same as the jaccard score in binary classification.\n",
    "\n",
    "For precision, this is the number of true positives out of all positives, both true and false. This is how often the\n",
    "model's predictions are correct from all of the positive predictions that it makes. This can show when we are making incorrect\n",
    "true predictions.\n",
    "\n",
    "For recall, this is the number of true positives out of all true positives and false negatives. This calculates how many times\n",
    "our model predicts a positive correctly out of all actual positives. This can show when we are missing true predictions. \n",
    "\n",
    "For f1 score, this is a function of both precision and recall. This helps when there is a need to balance precision and recall\n",
    "while their is an uneven class distribution. \n",
    "\n",
    "Accuracy is not a good metric to measure classification performance. If 77.5% of the data set is class 1 \n",
    "and the decision tree predicts class 1 100% of the time, this would also lead to 0.775 accuracy score but with a \n",
    "poor prediction model. Therefore, accuracy is not a good metric to reflect classification performance.  \"\"\"\n",
    "\n",
    "print(answer_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Overfitting issues\n",
    "<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font> Please re-train the decision tree model with a smaller number of training samples. <font color=red>DO NOT change the tree model parameters (e.g., minimum leaf node size) from Part 1</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   near infra red   red  green  blue  class\n",
      "0              143  150    145   156      0\n",
      "1              151  143    120   183      1\n",
      "2              100   98     86   156      0\n",
      "3              140  137    119   182      0\n",
      "4              147  147    138   160      0\n",
      "   near infra red   red  green  blue  class\n",
      "0              137  140    129   150      0\n",
      "1              169  162    140   193      1\n",
      "2              124  110     89   162      1\n",
      "3              105  104     99   153      1\n",
      "4              105  102     88   173      1\n",
      "0.701\n",
      "0.6588785046728972\n",
      "0.6482758620689655\n",
      "0.6535341830822711\n"
     ]
    }
   ],
   "source": [
    "# Re-run the training and testing based on a smaller training set (smalltrain.csv).\n",
    "#note: class 0 for dry land, class 1 for wetland\n",
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "# load dataset\n",
    "train_dat = pd.read_csv(\"smalltrain.csv\", header=None, names=col_names)\n",
    "print(train_dat.head())\n",
    "\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "print(test_dat.head())\n",
    "\n",
    "# COMPLETE CODES BELOW, compute training features X_train, training labels Y_train\n",
    "X_train = train_dat.drop('class', axis=1)\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "# COMPLETE CODES BELOW, compute test features X_test, test labels Y_test\n",
    "X_test = test_dat.drop('class', axis=1)\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "\n",
    "# Re-evaluate the trained model on test data, print out accuracy, precision, recall, F-score\n",
    "dt = DecisionTreeClassifier()\n",
    "dt = dt.fit(X_train,Y_train)\n",
    "Y_pred = dt.predict(X_test)\n",
    "\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "print(accuracy)\n",
    "print(precision_wet)\n",
    "print(recall_wet)\n",
    "print(F_wet)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>ANSWER THE QUESTION BELOW.</font> What do you observe when the number of training samples decrese? Is this overfitting or underfitting?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy, precision, recall, and f1 score all decreased when using a smaller training data set.\n",
      "This points to overfitting. The model has overadjusted and in essence memorized the training data. Now the predictions\n",
      "have worsened on test data as we have added noise from the smaller set into our prediction.\n"
     ]
    }
   ],
   "source": [
    "answer_2 = \"\"\"Accuracy, precision, recall, and f1 score all decreased when using a smaller training data set.\n",
    "This points to overfitting. The model has overadjusted and in essence memorized the training data. Now the predictions\n",
    "have worsened on test data as we have added noise from the smaller set into our prediction.\"\"\"\n",
    "\n",
    "print(answer_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try to increase model complexity yourself. For example, you can observe overfitting by increase the size of decision tree model. Please explore this yourself. It will <b>not be graded or required in this homework</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Compare different model on the same test data\n",
    "In this part, you will train other types of models and evaluate on the test data. You will compare their classification performance.\n",
    "\n",
    "<font color=red>PLEASE COMPLETE TEH CODES BELOW.</font> PLEASE USE THE SAME TRAINING AND TEST DATA as in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.756\n",
      "0.7301204819277108\n",
      "0.696551724137931\n",
      "0.7129411764705882\n"
     ]
    }
   ],
   "source": [
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "\n",
    "X_train = train_dat.drop('class', axis=1)\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "X_test = test_dat.drop('class', axis=1)\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "\n",
    "# train a logistic regression model; e.g., use the LogisticRegression model in Scikit-Learn.\n",
    "lr = LogisticRegression(solver='lbfgs')\n",
    "lr.fit(X_train, Y_train)\n",
    "Y_pred = lr.predict(X_test)\n",
    "\n",
    "\n",
    "# evalute the logistic regression model on test data\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "print(accuracy)\n",
    "print(precision_wet)\n",
    "print(recall_wet)\n",
    "print(F_wet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.773\n",
      "0.7666666666666667\n",
      "0.6873563218390805\n",
      "0.7248484848484849\n"
     ]
    }
   ],
   "source": [
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "\n",
    "X_train = train_dat.drop('class', axis=1)\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "X_test = test_dat.drop('class', axis=1)\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "\n",
    "# train a Support Vector Machine (SVM) model, e.g., the SVC model in Scikit-Learn, choose parameters approprioately\n",
    "svm_classifier = SVC(kernel='linear')\n",
    "svm_classifier.fit(X_train, Y_train)\n",
    "Y_pred = svm_classifier.predict(X_test)\n",
    "\n",
    "\n",
    "# evalute the Support Vector Machine (SVM) model on test data\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "print(accuracy)\n",
    "print(precision_wet)\n",
    "print(recall_wet)\n",
    "print(F_wet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=red>ANSWER THE QUESTION BELOW.</font> How do you compare the results from different models above with decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model:\n",
      "    In this model, we can observe a lower accuracy, very similar but slightly lower precision, lower recall and lower f1 score.\n",
      "    Recall is lower than precision as opposed to our decision tree, where the opposite effect was observed. \n",
      "    \n",
      "Support Vector Machine:\n",
      "    In this model, we can observe a very similar but slightly lower accuracy, higher precision, lower recall, and lower \n",
      "    f1 score. This model is more hesitant to make positive assumptions but does so more correctly.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_3 = \"\"\"Logistic Regression Model:\n",
    "    In this model, we can observe a lower accuracy, very similar but slightly lower precision, lower recall and lower f1 score.\n",
    "    Recall is lower than precision as opposed to our decision tree, where the opposite effect was observed. \n",
    "    \n",
    "Support Vector Machine:\n",
    "    In this model, we can observe a very similar but slightly lower accuracy, higher precision, lower recall, and lower \n",
    "    f1 score. This model is more hesitant to make positive assumptions but does so more correctly.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "print(answer_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Ensemble learning\n",
    "In this part, you will run several ensemble learning of decision trees, including bagging and random forest. For random forest, you can directly call it as a separate model from library.\n",
    "\n",
    "<font color=red>PLEASE COMPLETE CODES BELOW</font>. PLEASE USE THE ORIGINAL TRAINING AND TEST DATA in Part 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.804\n",
      "0.7825059101654847\n",
      "0.7609195402298851\n",
      "0.7715617715617716\n"
     ]
    }
   ],
   "source": [
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "\n",
    "X_train = train_dat.drop('class', axis=1)\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "X_test = test_dat.drop('class', axis=1)\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "\n",
    "# please train bagging of decision tree\n",
    "bc = BaggingClassifier()\n",
    "bc.fit(X_train, Y_train)\n",
    "Y_pred = bc.predict(X_test)\n",
    "\n",
    "\n",
    "# please evaluate it on the test data\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "print(accuracy)\n",
    "print(precision_wet)\n",
    "print(recall_wet)\n",
    "print(F_wet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.808\n",
      "0.7670329670329671\n",
      "0.8022988505747126\n",
      "0.7842696629213483\n"
     ]
    }
   ],
   "source": [
    "col_names = ['near infra red ', 'red', 'green', 'blue', 'class'] \n",
    "features =  ['near infra red ', 'red', 'green', 'blue'] \n",
    "\n",
    "train_dat = pd.read_csv(\"train.csv\", header=None, names=col_names)\n",
    "test_dat = pd.read_csv(\"test.csv\", header=None, names=col_names)\n",
    "\n",
    "X_train = train_dat.drop('class', axis=1)\n",
    "Y_train = train_dat['class']\n",
    "\n",
    "X_test = test_dat.drop('class', axis=1)\n",
    "Y_test = test_dat['class']\n",
    "\n",
    "\n",
    "# please train a random forest\n",
    "rf = RandomForestClassifier(n_estimators=100)\n",
    "rf.fit(X_train, Y_train)\n",
    "Y_pred = rf.predict(X_test)\n",
    "\n",
    "# please evaluate it on the test data\n",
    "accuracy = metrics.accuracy_score(Y_test, Y_pred)\n",
    "precision_wet = metrics.precision_score(Y_test, Y_pred)\n",
    "recall_wet = metrics.recall_score(Y_test, Y_pred)\n",
    "F_wet = metrics.f1_score(Y_test, Y_pred)\n",
    "print(accuracy)\n",
    "print(precision_wet)\n",
    "print(recall_wet)\n",
    "print(F_wet)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PLEASE ANSWER THE QUESTION BELOW. How do you compare different ensemble methods? Which one has the best performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging\n",
      "    This model has the second highest accuracy, highest precision, moderate recall,\n",
      "    and second highest f1 score. \n",
      "    \n",
      "Random Forest\n",
      "    This model had the highest accuracy, second highest precision, highest recall,\n",
      "    and highest f1 score. \n",
      "    \n",
      "Random Forest showed the overall best performance out of all different ensemble methods. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "answer_4 = \"\"\"Bagging\n",
    "    This model has the second highest accuracy, highest precision, moderate recall,\n",
    "    and second highest f1 score. \n",
    "    \n",
    "Random Forest\n",
    "    This model had the highest accuracy, second highest precision, highest recall,\n",
    "    and highest f1 score. \n",
    "    \n",
    "Random Forest showed the overall best performance out of all different ensemble methods. \n",
    "\"\"\"\n",
    "\n",
    "print(answer_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5 (EXTRA CREDIT): Implement your own classifier (logistic regression)\n",
    "\n",
    "You should implement your logistic regression model: a training function and a test function. Save your source codes in 'myLR.py'. Put the python script in the same dirctory of this Jupyter Notebook. Load the scripts so that you can call your own training and prediction functions. Then evaluate the results on test data. Compare your results from the results from built-in logistic regression library in Part 4. Please use the same training data and test data as Part 1. Please make sure you print out the accuracy, confusion matrix, precision, recall, F-score. \n",
    "\n",
    "<b>Requirement</b>\n",
    "\n",
    "- Your codes should run through without bugs. Codes with bugs in Jupyter notebook running will NOT be graded. \n",
    "    \n",
    "- You CANNOT copy a same python codes from online for this question. It will be treated as cheating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your implemented codes myLR.py\n",
    "\n",
    "\n",
    "# Call your own function to train a logistic regression model, as what we did in Part 1 with decision tree\n",
    "\n",
    "\n",
    "\n",
    "# Call your own function to make prediction on test data, and evaluate those metrics, as what we did in Part 1 with deicsion tree\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
